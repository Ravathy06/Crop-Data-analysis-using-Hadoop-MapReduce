{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiFATc-w4uXy"
      },
      "outputs": [],
      "source": [
        "#Download Hadoop\n",
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n",
        "\n",
        "#Extract the downloaded file:\n",
        "#we’ll use the tar command with the -x flag to extract, -z to uncompress, -v for verbose output, and\n",
        "#-f to specify that we’re extracting from a file\n",
        "!tar -xzvf hadoop-3.3.4.tar.gz\n",
        "\n",
        "#copy hadoop file to user/local\n",
        "!cp -r hadoop-3.3.4/ /usr/local/\n",
        "\n",
        "#2- Configuring Hadoop’s Java Home\n",
        "#To find the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "\n",
        "#Use the path highlighted above to set the default Java path that will be used by Hadoop\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "\n",
        "#3- Running Hadoop\n",
        "\n",
        "#Start running Hadoop\n",
        "!/usr/local/hadoop-3.3.4/bin/hadoop\n",
        "\n",
        "#Create the input folder in HDFS, (it will be inside root/input)\n",
        "!mkdir ~/input\n",
        "\n",
        "#check the contents of input folder\n",
        "!ls ~/input\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount (connect to) Google drive to be able to read from it (copy data files into HDFS)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ftzpfsc48b8",
        "outputId": "26fce4bc-b1dc-4e75-d18a-ef509a1f9965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Copy the data file into the input folder we created in HDFS\n",
        "!cp /content/drive/My\\ Drive/Hadoop/Crop_Yield.txt ~/input\n",
        "!cp /content/drive/My\\ Drive/Hadoop/Crop_Rainfall.txt ~/input"
      ],
      "metadata": {
        "id": "dQOhIWdA49ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Copy the Jar file into the input folder we created in HDFS\n",
        "!cp /content/drive/My\\ Drive/Hadoop/Crop.jar ~/input"
      ],
      "metadata": {
        "id": "jGCQe6ii5GLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the Crop.jar file on the data file (Crop_Yield.txt and Crop_Rainfall.txt) in the input folder\n",
        "# Assam<- filter value\n",
        "\n",
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar ~/input/Crop.jar ~/input/Crop_Yield.txt ~/input/Crop_Rainfall.txt ~/grep_example Assam\n"
      ],
      "metadata": {
        "id": "s5te2ymaTWE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the WordCount.jar file on the data file (pg4400.txt) file in the input folder\n",
        "#The output shows the word counts for the every word in the (pg4400.txt) file in the input folder\n",
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar ~/input/part.jar grep ~/input/finalcrop.txt ~/grep_example Gujarat\n",
        "\n"
      ],
      "metadata": {
        "id": "JAzXQBIo5YhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the contents of input folder again\n",
        "#The output shows the files we copied from Google Drive into the input folder\n",
        "!ls ~/input"
      ],
      "metadata": {
        "id": "ltCVPMBIp4DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the contents of the output file in the grep_example folder\n",
        "!cat ~/grep_example/part-r-00001"
      ],
      "metadata": {
        "id": "GxmYfR4BprKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List the contents of the grep_example folder\n",
        "#The output shows the output file named where the output data is stored\n",
        "!ls ~/grep_example/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP-dczifkrYI",
        "outputId": "e0e4a02b-0f45-475d-898b-2a14cb8bb1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-r-00000  part-r-00002  part-r-00004  _SUCCESS\n",
            "part-r-00001  part-r-00003  part-r-00005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want, you can copy the output file back from output folder grep_example to Google Drive\n",
        "!cp ~/grep_example/* /content/drive/My\\ Drive/"
      ],
      "metadata": {
        "id": "02LfbIsxksSZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}